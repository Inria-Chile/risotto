{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Topic Classification on CORD-19\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll build a Zero Shot Topic Classifier on the COVID-19 Open Research Dataset (CORD-19, Wang et al., 2020).\n",
    "Essentially, we aim to build a web application capable of receiving natural language questions, such as \"what do we know about vaccines and therapeutics?\", and then displaying the most relevant research literature regarding the specific question.\n",
    "This dataset has received wide attention in the data mining and natural language processing community in order to develop tools to aid health workers stay up-to-date with the latest and most relevant research about the current pandemic.\n",
    "\n",
    "Recent advances in NLP, such as OpenAI's GPT-3 (Brown et al., 2020), have shown that large language models can achieve competitive performance on downstream tasks with less task-specific data than it'd be required by smaller models.\n",
    "However, GPT-3 is currently difficult to use on real world applications due to its size of ~175 billions of parameters.\n",
    "\n",
    "Recent experiments made at HuggingFace (Davison, 2020) explored the potential of using Sentence-BERT (Reimers and Gurevych, 2020) to separately embed sentences and never-seen-before topic labels.\n",
    "Then, they'd rank the sentence's topics by measuring the cosine distance between both vectors (Veeranna, 2016), obtaining promising results.\n",
    "\n",
    "In another experiment, they use a pre-trained natural languange inference (NLI) sequence-pair classifier as an out of-the-box zero shot text classifier, as proposed by Yin et al. (2020).\n",
    "By using a pre-trained BART model (Lewis et al., 2019) fine-tuned on the Multigenre NLI corpus, they were able to score an F1 score of 53.7 on the Yahoo News dataset.\n",
    "The dataset has 10 classes and the current supervised models state of the art is an accuracy of 77.62.\n",
    "\n",
    "## Cosine similarity method\n",
    "\n",
    "First, we'll use Sentence-BERT to embed both the papers and the never-seen-before question in order to measure the cosine similarity and assess the papers relevance to the question.\n",
    "For the sake of efficiency, we'll iterate over the dataset and precompute the papers representations using their title and abstract.\n",
    "Then, we'll compute the cosine distance between the embedded question and each one of the papers representations.\n",
    "Finally, the papers might be ordered by their distances in order to first display those that are relevant to the question.\n",
    "\n",
    "This method has been implemented at https://risotto.inria.cl/zero-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4,5,6\n"
     ]
    }
   ],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=4,5,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "\n",
    "def embed_papers(papers):\n",
    "    # papers = load_papers_artifact()\n",
    "    model = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "    batch_size = 256\n",
    "    num_rows = len(papers)\n",
    "    num_batches = math.ceil(num_rows / batch_size)\n",
    "\n",
    "    embeddings = pd.Series([], dtype=object, name=\"embeddings\")\n",
    "\n",
    "    for batch_id in progress_bar(range(num_batches)):\n",
    "        # Concatenate title and abstract\n",
    "        start_idx = batch_id * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        slice_df = papers.iloc[start_idx:end_idx]\n",
    "        title_abstract = (slice_df.title + \". \" +\n",
    "                          slice_df.abstract).fillna(\"\").values.tolist()\n",
    "\n",
    "        # Embed\n",
    "        sentence_embeddings = model.encode(title_abstract)\n",
    "\n",
    "        for i, (paper_idx, _) in enumerate(slice_df.iterrows()):\n",
    "            embeddings.at[paper_idx] = sentence_embeddings[i]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def cosine_distance_query(model, query, papers_embeddings):\n",
    "    query_encoded = model.encode([query])\n",
    "    papers_array = np.array(papers_embeddings.to_list())\n",
    "    distances = scipy.spatial.distance.cdist(query_encoded, papers_array,\n",
    "                                             \"cosine\")[0]\n",
    "    distances_series = pd.Series(distances,\n",
    "                                 index=papers_embeddings.index,\n",
    "                                 name=\"distances\")\n",
    "\n",
    "    return distances_series\n",
    "\n",
    "\n",
    "def get_sentence_transformer(name=\"bert-base-nli-mean-tokens\"):\n",
    "    return SentenceTransformer(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Inference (NLI) method\n",
    "\n",
    "In this approach we use a BART classifier (Lewis et al., 2019) pre-trained on the Multi-Genre NLI (MultiNLI, Williams et al., 2018) corpus as the base model.\n",
    "\n",
    "Given research interests expressed in natural language, we pose the problem of recovering relevant research from the CORD-19 dataset (Wang et al., 2020) as a Zero Shot Topic Classification task (Yin et al., 2019).\n",
    "Leveraging the Natural Language Inference task framework, we assess each paper relevance by feeding the model with the paper's title and abstract as premise and a research interest as hypothesis.\n",
    "\n",
    "Finally, we use the model's entailment inference values as proxy relevance scores for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmarti/risotto/venv-risotto/lib/python3.7/site-packages/spacy/util.py:271: UserWarning: [W031] Model 'en_core_sci_sm' (0.2.4) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.0). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import math\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "\n",
    "\n",
    "def get_nli_model(name=\"facebook/bart-large-mnli\"):\n",
    "    model = BartForSequenceClassification.from_pretrained(name)\n",
    "    tokenizer = BartTokenizer.from_pretrained(name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmarti/.pyenv/versions/3.8.2/envs/risotto/lib/python3.8/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/Users/lmarti/.pyenv/versions/3.8.2/envs/risotto/lib/python3.8/site-packages/spacy/util.py:271: UserWarning: [W031] Model 'en_core_sci_sm' (0.2.4) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.0). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is not ready.\n"
     ]
    }
   ],
   "source": [
    "from risotto.artifacts import load_papers_artifact\n",
    "\n",
    "try:\n",
    "    papers = load_papers_artifact()\n",
    "    model, tokenizer = get_nli_model()\n",
    "except:\n",
    "    print('Data is not ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import numpy as np\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "\n",
    "def build_tokenized_papers_artifact(papers,\n",
    "                                    tokenizer,\n",
    "                                    should_dump=True,\n",
    "                                    dump_path=None,\n",
    "                                    batch_size=128):\n",
    "    num_batches = math.ceil(len(papers) / batch_size)\n",
    "    tokenized_series = pd.Series([], dtype=object, name=\"tokenized_papers\")\n",
    "\n",
    "    for batch_idx in progress_bar(range(num_batches)):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        papers_batch = papers.iloc[start_idx:end_idx]\n",
    "\n",
    "        title_abstract = (papers_batch.title + \". \" +\n",
    "                          papers_batch.abstract).fillna(\" \").values.tolist()\n",
    "        tokenized_batch = tokenizer.batch_encode_plus(\n",
    "            title_abstract, max_length=tokenizer.model_max_length)\n",
    "\n",
    "        for i, (paper_idx, _) in enumerate(papers_batch.iterrows()):\n",
    "            tokenized_series.at[paper_idx] = tokenized_batch[\"input_ids\"][i]\n",
    "\n",
    "    if should_dump:\n",
    "        tokenized_series.to_hdf(dump_path, key=\"tokenized_papers\")\n",
    "\n",
    "    return tokenized_series\n",
    "\n",
    "\n",
    "def load_tokenized_papers_artifact(artifacts_path):\n",
    "    return pd.read_hdf(artifacts_path, key=\"tokenized_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='604' class='' max='604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [604/604 08:29<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ug7v899j    [0, 20868, 1575, 9, 2040, 12, 32012, 1308, 438...\n",
       "02tnwd4m    [0, 19272, 4063, 30629, 35, 10, 1759, 12, 3382...\n",
       "ejv2xln0    [0, 6544, 24905, 927, 8276, 12, 495, 8, 34049,...\n",
       "2b73a28n    [0, 21888, 9, 253, 15244, 2614, 12, 134, 11, 1...\n",
       "9785vg6d    [0, 13120, 8151, 11, 22201, 44828, 4590, 11, 1...\n",
       "Name: tokenized_papers, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build tokenized papers\n",
    "tokenized_papers = build_tokenized_papers_artifact(\n",
    "    papers=papers,\n",
    "    tokenizer=tokenizer,\n",
    "    dump_path=\"artifacts/nli_artifacts.hdf\")\n",
    "tokenized_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ug7v899j    [0, 20868, 1575, 9, 2040, 12, 32012, 1308, 438...\n",
       "02tnwd4m    [0, 19272, 4063, 30629, 35, 10, 1759, 12, 3382...\n",
       "ejv2xln0    [0, 6544, 24905, 927, 8276, 12, 495, 8, 34049,...\n",
       "2b73a28n    [0, 21888, 9, 253, 15244, 2614, 12, 134, 11, 1...\n",
       "9785vg6d    [0, 13120, 8151, 11, 22201, 44828, 4590, 11, 1...\n",
       "Name: tokenized_papers, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenized papers\n",
    "tokenized_papers = load_tokenized_papers_artifact(\"artifacts/nli_artifacts.hdf\")\n",
    "tokenized_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def build_entailments_artifact(tokenized_papers,\n",
    "                               query_tokenized,\n",
    "                               batch_size=64,\n",
    "                               device=\"cuda\",\n",
    "                               should_dump=True,\n",
    "                               dump_path=None):\n",
    "    # query_encoded = [2, 2, *tokenizer.encode(query)[1:]]\n",
    "    query_encoded = [2, 2, *query_tokenized[1:]]\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    num_batches = math.ceil(len(tokenized_papers) / batch_size)\n",
    "    entail_probs = []\n",
    "\n",
    "    for batch_idx in progress_bar(range(num_batches)):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        tokenized_papers_batch = tokenized_papers.iloc[\n",
    "            start_idx:end_idx].tolist()\n",
    "        max_length = float(\"-inf\")\n",
    "        for i in range(len(tokenized_papers_batch)):\n",
    "            tokenized_paper = tokenized_papers_batch[i]\n",
    "            # Ugly hack...\n",
    "            tokenized_papers_batch[i] = [\n",
    "                *tokenized_paper[:-1][:(tokenizer.model_max_length -\n",
    "                                        len(query_encoded))], *query_encoded\n",
    "            ]\n",
    "            tokenized_paper = tokenized_papers_batch[i]\n",
    "            if len(tokenized_paper) > max_length:\n",
    "                max_length = len(tokenized_paper)\n",
    "        masks = []\n",
    "        for tokenized_paper in tokenized_papers_batch:\n",
    "            paper_length = len(tokenized_paper)\n",
    "            delta = max_length - paper_length\n",
    "            tokenized_paper += [1 for _ in range(delta)]\n",
    "            mask = [1 for _ in range(paper_length)]\n",
    "            mask += [0 for _ in range(delta)]\n",
    "            masks.append(mask)\n",
    "\n",
    "        input_ids = torch.tensor(tokenized_papers_batch).to(device)\n",
    "        attention_mask = torch.tensor(masks).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "        entail_contradiction_logits = outputs[:, [0, 2]]\n",
    "        probs = entail_contradiction_logits.softmax(dim=1)\n",
    "        entail_probs += (probs[:, 1] * 100).tolist()\n",
    "\n",
    "    entail_series = pd.Series(entail_probs,\n",
    "                              index=tokenized_papers.index,\n",
    "                              name=\"entailments\")\n",
    "\n",
    "    if should_dump:\n",
    "        entail_series.to_hdf(dump_path, key=\"entailments\")\n",
    "\n",
    "    return entail_series\n",
    "\n",
    "\n",
    "def load_entailments_artifact(artifacts_path):\n",
    "    return pd.read_hdf(artifacts_path, key=\"entailments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. https://arxiv.org/abs/2005.14165\n",
    "\n",
    "Davison, J. (2020). Zero-Shot Learning in Modern NLP. https://joeddav.github.io/blog/2020/05/29/ZSL.html\n",
    "\n",
    "Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. http://arxiv.org/abs/1910.13461\n",
    "\n",
    "Reimers, N., & Gurevych, I. (2020). Sentence-BERT: Sentence embeddings using siamese BERT-networks. EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference, 3982–3992. https://doi.org/10.18653/v1/d19-1410\n",
    "\n",
    "Veeranna, S. P., Nam, J., Mencía, E. L., & Fürnkranz, J. (2016). Using semantic similarity for multi-label zero-shot classification of text documents. ESANN 2016 - 24th European Symposium on Artificial Neural Networks, April, 423–428.\n",
    "\n",
    "Wang, L. L., Lo, K., Chandrasekhar, Y., Reas, R., Yang, J., Eide, D., Funk, K., Kinney, R., Liu, Z., Merrill, W., Mooney, P., Murdick, D., Rishi, D., Sheehan, J., Shen, Z., Stilson, B., Wade, A. D., Wang, K., Wilhelm, C., … Kohlmeier, S. (2020). CORD-19: The Covid-19 Open Research Dataset. https://arxiv.org/abs/2004.10706\n",
    "\n",
    "Williams, A., Nangia, N., & Bowman, S. R. (2018). A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 1112--1122. http://aclweb.org/anthology/N18-1101\n",
    "\n",
    "Yin, W., Hay, J., & Roth, D. (2019). Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference, 3914–3923. https://doi.org/10.18653/v1/d19-1404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
