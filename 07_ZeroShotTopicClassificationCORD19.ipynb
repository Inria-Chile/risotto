{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Topic Classification on CORD-19\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll build a Zero Shot Topic Classifier on the COVID-19 Open Research Dataset (CORD-19, Wang et al., 2020).\n",
    "Essentially, we aim to build a web application capable of receiving natural language questions, such as \"what do we know about vaccines and therapeutics?\", and then displaying the most relevant research literature regarding the specific question.\n",
    "This dataset has received wide attention in the data mining and natural language processing community in order to develop tools to aid health workers stay up-to-date with the latest and most relevant research about the current pandemic.\n",
    "\n",
    "Recent advances in NLP, such as OpenAI's GPT-3 (Brown et al., 2020), have shown that large language models can achieve competitive performance on downstream tasks with less task-specific data than it'd be required by smaller models.\n",
    "However, GPT-3 is currently difficult to use on real world applications due to its size of ~175 billions of parameters.\n",
    "\n",
    "Recent experiments made at HuggingFace (Davison, 2020) explored the potential of using Sentence-BERT (Reimers and Gurevych, 2020) to separately embed sentences and never-seen-before topic labels.\n",
    "Then, they'd rank the sentence's topics by measuring the cosine distance between both vectors (Veeranna, 2016), obtaining promising results.\n",
    "\n",
    "In another experiment, they use a pre-trained natural languange inference (NLI) sequence-pair classifier as an out of-the-box zero shot text classifier, as proposed by Yin et al. (2020).\n",
    "By using a pre-trained BART model fine-tuned on the Multigenre NLI corpus, they were able to score an F1 score of 53.7 on the Yahoo News dataset.\n",
    "The dataset has 10 classes and the current supervised models state of the art is an accuracy of 77.62.\n",
    "\n",
    "## Proposed method\n",
    "\n",
    "First, we'll use Sentence-BERT to embed both the papers and the never-seen-before question in order to measure the cosine distance and assess the paper relevance to the question.\n",
    "For the sake of efficiency, we'll iterate over the dataset and precompute the papers representations using their title and abstract.\n",
    "Then, we'll compute the cosine distance between the embedded question and each one of the papers representations.\n",
    "Finally, the papers might be ordered by their distances in order to first display those that are relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmarti/risotto/venv-risotto/lib/python3.7/site-packages/spacy/util.py:271: UserWarning: [W031] Model 'en_core_sci_sm' (0.2.4) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.0). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='302' class='' max='302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [302/302 11:35<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "\n",
    "def embed_papers(papers):\n",
    "    # papers = load_papers_artifact()\n",
    "    model = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "    batch_size = 256\n",
    "    num_rows = len(papers)\n",
    "    num_batches = math.ceil(num_rows / batch_size)\n",
    "\n",
    "    embeddings = pd.Series([], dtype=object, name=\"embeddings\")\n",
    "\n",
    "    for batch_id in progress_bar(range(num_batches)):\n",
    "        # Concatenate title and abstract\n",
    "        start_idx = batch_id * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        slice_df = papers.iloc[start_idx:end_idx]\n",
    "        title_abstract = (slice_df.title + \". \" + slice_df.abstract).fillna(\"\").values.tolist()\n",
    "\n",
    "        # Embed\n",
    "        sentence_embeddings = model.encode(title_abstract)\n",
    "\n",
    "        for i, (paper_idx, _) in enumerate(slice_df.iterrows()):\n",
    "            embeddings.at[paper_idx] = sentence_embeddings[i]\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def cosine_distance_query(model, query, papers_embeddings):\n",
    "    query_encoded = model.encode([query])\n",
    "    papers_array = np.array(papers_embeddings.to_list())\n",
    "    distances = scipy.spatial.distance.cdist(query_encoded, papers_array, \"cosine\")[0]\n",
    "    distances_series = pd.Series(distances, index=papers.index, name=\"distances\")\n",
    "    \n",
    "    return distances_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. https://arxiv.org/abs/2005.14165\n",
    "- Davison, J. (2020). Zero-Shot Learning in Modern NLP. https://joeddav.github.io/blog/2020/05/29/ZSL.html\n",
    "- Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. http://arxiv.org/abs/1910.13461\n",
    "- Reimers, N., & Gurevych, I. (2020). Sentence-BERT: Sentence embeddings using siamese BERT-networks. EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference, 3982–3992. https://doi.org/10.18653/v1/d19-1410\n",
    "- Veeranna, S. P., Nam, J., Mencía, E. L., & Fürnkranz, J. (2016). Using semantic similarity for multi-label zero-shot classification of text documents. ESANN 2016 - 24th European Symposium on Artificial Neural Networks, April, 423–428.\n",
    "- Wang, L. L., Lo, K., Chandrasekhar, Y., Reas, R., Yang, J., Eide, D., Funk, K., Kinney, R., Liu, Z., Merrill, W., Mooney, P., Murdick, D., Rishi, D., Sheehan, J., Shen, Z., Stilson, B., Wade, A. D., Wang, K., Wilhelm, C., … Kohlmeier, S. (2020). CORD-19: The Covid-19 Open Research Dataset. https://arxiv.org/abs/2004.10706\n",
    "- Yin, W., Hay, J., & Roth, D. (2020). Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference, 3914–3923. https://doi.org/10.18653/v1/d19-1404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
