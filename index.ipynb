{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš RISOTTO\n",
    "> Research Intelligent Support and Organization TOol against COVID-19 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python 3.x](https://img.shields.io/badge/python-3.x-green.svg)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Inria-Chile/risotto)\n",
    "[![License: CeCILL-B](https://img.shields.io/badge/license-CeCILL--B-orange)](https://cecill.info/licences.en.html)\n",
    "[![Fighting against COVID-19](https://img.shields.io/badge/fighting-%F0%9F%A6%A0COVID--19-9cf)]()\n",
    "\n",
    "# About RISOTTO\n",
    "\n",
    "The [COVID-19](https://en.wikipedia.org/wiki/Coronavirus_disease_2019) research effort is generating a massive amount of information that is reported as research papers. Traditional human-annotated, ontology-based or supervised (tag-based) learning NLP methods are not able to handle the task of supporting research and understanding results adequately.\n",
    "\n",
    "So far, to the best of our knowledge, there is not a research support tool that exploits the current state of the art of unsupervised learning in NLP that could be used to provide a quick response to this problem.\n",
    "\n",
    "RISOTTO is a research support tool that applies state of the art unsupervised NLP and ML methods to:\n",
    "- analyze COVID-19-related research papers freely available online,\n",
    "- automatically detect hierarchical groups of these papers and topics,\n",
    "- present leading \"popular\" papers as well as potential breakthroughs and new results, and\n",
    "- provide a visualization tool to understand how is the progress doing what areas are receiving more attention in time, etc.\n",
    "\n",
    "At the moment, use as input the research papers that have been consolidated as the [CORD-19 dataset](https://www.semanticscholar.org/cord19) by the [Allen Institute for AI](https://allenai.org) and collaborating institutions. [1]\n",
    "\n",
    "**Note:** It is expected that, as we progress on the conception and development of this tool new tasks/tools will be added to this list.\n",
    "\n",
    "### References\n",
    "\n",
    "1. Wang, L.L., Lo, K., Chandrasekhar, Y., Reas, R., Yang, J., Eide, D., Funk, K., Kinney, R.M., Liu, Z., Merrill, W., Mooney, P., Murdick, D.A., Rishi, D., Sheehan, J., Shen, Z., Stilson, B., Wade, A.D., Wang, K., Wilhelm, C., Xie, B., Raymond, D.M., Weld, D.S., Etzioni, O., & Kohlmeier, S. (2020). CORD-19: The Covid-19 Open Research Dataset. *ArXiv, [abs/2004.10706.](https://arxiv.org/pdf/2004.10706.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing RISOTTO\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "In order to use RISOTTO you will need a [Kaggle](https://www.kaggle.com) username and key.\n",
    "\n",
    "### Get a Kaggle API key\n",
    "\n",
    "If you do not have a Kaggle api key, you may follow the instructions here: https://www.kaggle.com/docs/api. You can set your API key by either:\n",
    "\n",
    "- Copying the `kaggle.json` file in your local system, in `~/.kaggle/kaggle.json` for Linux/Mac or `C:\\Users<Windows-username>.kaggle\\kaggle.json` for Windows.\n",
    "- Setting the `KAGGLE_USERNAME` and `KAGGLE_KEY` environment variables by one of the following options:\n",
    "\n",
    "```bash\n",
    "# Temporarily (you will need to do this everytime you open a new terminal):\n",
    "export $KAGGLE_USERNAME=<your_username>\n",
    "export $KAGGLE_KEY=<your_key>\n",
    "```\n",
    "\n",
    "```bash\n",
    "# For bash\n",
    "echo 'export KAGGLE_USERNAME=<your_username>' >> ~/.bash_profile\n",
    "echo 'export KAGGLE_KEY=<your_key>' >> ~/.bash_profile\n",
    "```\n",
    "\n",
    "```bash\n",
    "# For zsh\n",
    "echo 'export KAGGLE_USERNAME=<your_username>' >> ~/.zshenv\n",
    "echo 'export KAGGLE_KEY=<your_key>' >> ~/.zshenv\n",
    "```\n",
    "\n",
    "- Setting the `KAGGLE_USERNAME` and `KAGGLE_KEY` environment variables in a `.env` file (for Docker use only!)\n",
    "\n",
    "```bash\n",
    "touch .env\n",
    "echo 'KAGGLE_USERNAME=<your_username>' >> .env\n",
    "echo 'KAGGLE_KEY=<your_key>' >> .env\n",
    "```\n",
    "\n",
    "### Clone or download the repository\n",
    "\n",
    "```bash\n",
    "git clone git@github.com:Inria-Chile/risotto.git\n",
    "cd risotto\n",
    "```\n",
    "\n",
    "## How to use RISOTTO locally (without docker)\n",
    "\n",
    "### Build virtual environment with python3 and install requirements\n",
    "\n",
    "```bash\n",
    "python -m virtualenv -p python3 venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Run the build and preprocess scripts\n",
    "\n",
    "The build script converts the jupyter notebooks to python scripts. The `preprocess.py` script downloads the dataset and builds the artifacts if it is necessary, the `-f` flag forces to re-download and re-build.\n",
    "\n",
    "```bash\n",
    "source venv/bin/activate # If you have not sourced the virtual environment already\n",
    "python scripts/build.py\n",
    "python scripts/preprocess.py -f # Use the -f flag to override dataset and artifact ts if they exist\n",
    "```\n",
    "\n",
    "### Run the GUI\n",
    "\n",
    "The GUI will be available at localhost:8000\n",
    "\n",
    "```bash\n",
    "source venv/bin/activate # If you have not sourced the virtual environment already\n",
    "voila --port=8000 --no-browser --enable_nbextensions=True 06_GUI.ipynb\n",
    "```\n",
    "\n",
    "## How to use RISOTTO with docker\n",
    "\n",
    "We provide 2 sets of docker files to serve 2 different purposes: development and production\n",
    "\n",
    "### Docker for development\n",
    "\n",
    "This option mounts the whole repository inside the docker container, in order to facilitate using changes in the code\n",
    "\n",
    "```bash\n",
    "docker-compose -f docker-compose-dev.yml build\n",
    "docker-compose -f docker-compose-dev.yml up -d\n",
    "\n",
    "# To restart the container:\n",
    "docker-compose -f docker-compose-dev.yml restart risotto\n",
    "docker-compose -f docker-compose-dev.yml up -d\n",
    "\n",
    "# To force update the dataset and artifacts:\n",
    "docker-compose -f docker-compose-dev.yml exec risotto python scripts/preprocess.py -f\n",
    "\n",
    "# To stop the container:\n",
    "docker-compose -f docker-compose-dev.yml down -v\n",
    "```\n",
    "\n",
    "### Docker for production\n",
    "\n",
    "This option only mounts the dataset and artifacts folders, in order to keep them out of the container\n",
    "\n",
    "```bash\n",
    "docker-compose build\n",
    "docker-compose up -d\n",
    "\n",
    "# To restart the container:\n",
    "docker-compose restart risotto\n",
    "docker-compose up -d\n",
    "\n",
    "# To force update the dataset and artifacts:\n",
    "docker-compose exec risotto python scripts/preprocess.py -f\n",
    "\n",
    "# To stop the container:\n",
    "docker-compose down -v\n",
    "```\n",
    "\n",
    "```bash\n",
    "docker-compose build\n",
    "docker-compose up -d\n",
    "# To stop the container:\n",
    "docker-compose down -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to contribute\n",
    "\n",
    "## How to get started\n",
    "\n",
    "Before anything else, please install the `nbdev` git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\n",
    "```\n",
    "nbdev_install_git_hooks\n",
    "```\n",
    "\n",
    "## Did you find a bug?\n",
    "\n",
    "* Ensure the bug was not already reported by searching on GitHub under Issues.\n",
    "* If you're unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\n",
    "* Be sure to add the complete error messages.\n",
    "\n",
    "#### Did you write a patch that fixes a bug?\n",
    "\n",
    "* Open a new GitHub pull request with the patch.\n",
    "* Ensure that your PR includes a test that fails without your patch, and pass with it.\n",
    "* Ensure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n",
    "\n",
    "\n",
    "## PR submission guidelines\n",
    "\n",
    "* Keep each PR focused. While it's more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\n",
    "* Do not mix style changes/fixes with \"functional\" changes. It's very difficult to review such PRs and it most likely get rejected.\n",
    "* Do not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\n",
    "* Do not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\n",
    "* If, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won't need to review the whole PR again. In the exception case where you realize it'll take many many commits to complete the requests, then it's probably best to close the PR, do the work and then submit it again. Use common sense where you'd choose one way over another.\n",
    "\n",
    "\n",
    "## Do you want to contribute to the documentation?\n",
    "\n",
    "* Docs are automatically created from the notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
