{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='left' style=\"width:29%;overflow:hidden;\">\n",
    "<a href='http://inria.fr'>\n",
    "<img src='https://github.com/lmarti/jupyter_custom/raw/master/imgs/inr_logo_rouge.png' alt='Inria logo' title='Inria logo'/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Topic Modeling\n",
    "\n",
    "> In this notebook we're going to expand our previous topic modeling approaches in order to model hierarchic topics.\n",
    "Even though a single level topic modeling is helpful to go over the vast amount of papers in the CORD-19 dataset, our hypothesis is that a hierarchical topic modeling will provide a much more easier way to sway through the papers.\n",
    "\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "The following references have caught our attention:\n",
    "\n",
    "- https://radimrehurek.com/gensim/models/hdpmodel.html: an implementation of Hierarchical Dirichlet Processes (HDP) using the topic modelling library `gensim`.\n",
    "- https://github.com/joewandy/hlda: an implementation of Hierarchical Latent Dirichlet Allocation (hLDA) in Python.\n",
    "- https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process: a comparison between LDA and HDP.\n",
    "- https://developer.squareup.com/blog/inferring-label-hierarchies-with-hlda/: a write up about Square's experience using hLDA to hierarchically classify customer support articles.\n",
    "- https://www.hindawi.com/journals/sp/2017/4382348/: a journal article.\n",
    "\n",
    "Preliminary notes:\n",
    "\n",
    "- LDA models documents as Dirichlet mixtures of a fixed number of topics, which are modeled as Dirichlet mixtures of words.\n",
    "- hLDA is an adaptation of LDA that models topics as a mixture of a new, distinct level of topics.\n",
    "- HDP main difference with respect to LDA is that the number of topics isn't an hyperparamenter, but is discarded because it doesn't build a hierarchical topic structure.\n",
    "\n",
    "First, we'll try a hierarchical topic modeling using hLDA and then we'll compare it to a manual LDA hierarchical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Latent Dirichlet Allocation (hLDA)\n",
    "\n",
    "This technique was presented in the 2004 NeurIPS paper \"Hierarchical Topic Models and the Nested Chinese Restaurant Process\" by David Blei et al. available at: https://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf.\n",
    "\n",
    "A quick Google search yields at least two implementations:\n",
    "\n",
    "- https://github.com/blei-lab/hlda: implemented in C by the original authors. Last commit was in 2014.\n",
    "- https://github.com/joewandy/hlda: implemented in Python. Last commit was in 2017.\n",
    "\n",
    "We'll use the second one since it publishes a Jupyter Notebook with an example using the library.\n",
    "First, we'll install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp hierarchical_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "from risotto.lda import tokenizer\n",
    "from risotto.references import load_papers_from_metadata_file, paper_as_markdown\n",
    "from risotto.lda import process_papers_file_contents\n",
    "from risotto.hlda_sampler import HierarchicalLDA\n",
    "from risotto.lda import print_topic_words\n",
    "\n",
    "import pickle, random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports not required by the library\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll proceed to load the CORD-19 dataset papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORD19_DATASET_FOLDER = Path(\"./datasets/CORD-19-research-challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers, _ = load_papers_from_metadata_file(CORD19_DATASET_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've loaded the papers on memory.\n",
    "Now, we'll process them in order to produce text strings with their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = process_papers_file_contents(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last preprocessing step is to vectorize each paper.\n",
    "We'll represent them using the CountVectorizer `scikit-learn` implementation.\n",
    "We purposefully don't use representations such as `tf-idf` because the LDA algorithm takes care of the document frequency normalization of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hlda_corpus(docs, max_vocab_size=2**13):\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        lowercase=True,\n",
    "        max_features=max_vocab_size,\n",
    "    )\n",
    "    \n",
    "    count_vectorizer.fit(docs)\n",
    "    \n",
    "    vocab = count_vectorizer.vocabulary_\n",
    "    docs_tokenized = []\n",
    "    docs_tokens_idxs = []\n",
    "    \n",
    "    for doc in progress_bar(docs):\n",
    "        tokens = [token.lower() for token in tokenizer(doc)]\n",
    "        idxs = []\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                idxs.append(vocab[token])\n",
    "        docs_tokenized.append(tokens)\n",
    "        docs_tokens_idxs.append(idxs)\n",
    "        \n",
    "    vocab_list = count_vectorizer.get_feature_names()\n",
    "    \n",
    "    return docs_tokenized, docs_tokens_idxs, vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokenized, docs_tokens_idxs, vocab = get_hlda_corpus(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docs_tokenized` is a list of lists with the tokenization of each paper.\n",
    "`docs_tokens_idxs` is a list of lists with the vocabulary indeces of each paper token.\n",
    "Finally, `vocab` is the list with the vocabulary used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlda = HierarchicalLDA(\n",
    "    corpus=random.sample(docs_tokens_idxs, int(len(docs_tokens_idxs) * 0.1)),\n",
    "    vocab=vocab,\n",
    "    alpha=10,\n",
    "    gamma=1,\n",
    "    eta=0.1,\n",
    "    seed=0,\n",
    "    verbose=True,\n",
    "    num_levels=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlda.estimate(\n",
    "    num_samples=500,\n",
    "    display_topics=50,\n",
    "    n_words=5,\n",
    "    with_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling a 10% of the total papers results in a sub-dataset of about 3.888 papers.\n",
    "The number of topics of each level is determined by the Chinese Restaurant Process and can be influenced by tweaking the `alpha` and `gamma` hyperparameters.\n",
    "Training the model on the 10% sample took about an hour for each 50 iterations.\n",
    "\n",
    "To avoid spending time retraining the model, it'll be dumped to be able to load it in posterior experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the model\n",
    "with open(\"hlda.pkl\", \"wb\") as dump_file:\n",
    "    pickle.dump(hlda, dump_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the dumped model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open(\"hlda.pkl\", \"rb\") as dump_file:\n",
    "    hlda = pickle.load(dump_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Hierarchical LDA\n",
    "\n",
    "In this section we'll attempt to manually build a hierarchical topic model.\n",
    "Essentially, using the same number of topics found by the hLDA technique at `level=1`, we'll model topics using the standard LDA.\n",
    "Afterwards, for each group of documents of the first level topics, we'll run a new LDA topic modelling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def get_lda_corpus(docs, max_vocab_size=2**13):\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        lowercase=True,\n",
    "        max_features=max_vocab_size,\n",
    "    )\n",
    "    vectorized_docs = count_vectorizer.fit_transform(docs)\n",
    "    return vectorized_docs, count_vectorizer\n",
    "\n",
    "\n",
    "def fit_lda_model(docs, **kwargs):\n",
    "    vectorized_docs, count_vectorizer = get_lda_corpus(docs)\n",
    "    lda = LatentDirichletAllocation(**kwargs)\n",
    "    lda = lda.fit(vectorized_docs)\n",
    "    return lda, vectorized_docs, count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda, vectorized_docs, count_vectorizer = fit_lda_model(\n",
    "    docs,\n",
    "    n_components=8,\n",
    "    verbose=2,\n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will print the most relevant tokens of each modelled component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topic_words(lda, count_vectorizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll build the groups of papers belonging to the different modelled topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_docs_by_topics(model, vectorized_docs):\n",
    "    docs_classified = lda.transform(vectorized_docs)\n",
    "    docs_topics = docs_classified.argmax(1)\n",
    "    clustered_docs = defaultdict(list)\n",
    "\n",
    "    for vectorized_doc, topic_idx in zip(vectorized_docs, docs_topics):\n",
    "        clustered_docs[topic_idx].append(vectorized_doc)\n",
    "\n",
    "    stacked_clustered_docs = {}\n",
    "    for topic_idx, docs_list in clustered_docs.items():\n",
    "        stacked_clustered_docs[topic_idx] = vstack(docs_list)\n",
    "    \n",
    "    return stacked_clustered_docs\n",
    "\n",
    "grouped_docs = group_docs_by_topics(lda, vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each paper group, we'll run LDA on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topic_words(lda, count_vectorizer, 5)\n",
    "\n",
    "models = {}\n",
    "for topic_idx, group_docs in progress_bar(grouped_docs.items()):\n",
    "    print(f\"Topic ID #{topic_idx}; documents = {group_docs.shape[0]}\")\n",
    "    \n",
    "    models[topic_idx] = LatentDirichletAllocation(\n",
    "        n_components=4,\n",
    "        verbose=0,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    models[topic_idx] = models[topic_idx].fit(group_docs)\n",
    "    \n",
    "    print_topic_words(models[topic_idx], count_vectorizer, 5)\n",
    "    print(\"\\n\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
