{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='left' style=\"width:29%;overflow:hidden;\">\n",
    "<a href='http://inria.fr'>\n",
    "<img src='https://github.com/lmarti/jupyter_custom/raw/master/imgs/inr_logo_rouge.png' alt='Inria logo' title='Inria logo'/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representations, LDA and topics in CORD-19\n",
    "\n",
    "> Here we calculate representations of the papers based on their text content. Then, from these representations, a modeling of topics will be carried out using the LDA method. Finally, the most relevant papers for each topic will be determined using the PageRank scores of each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from risotto.references import load_papers_from_metadata_file, build_papers_reference_graph, paper_as_markdown\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import scispacy\n",
    "import en_core_sci_sm\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading paper dataset and re-generating the graph of papers and the corresponding PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord19_dataset_folder = \"./datasets/CORD-19-research-challenge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers, _ = load_papers_from_metadata_file(cord19_dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_papers_reference_graph(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageranks = nx.pagerank(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper representations\n",
    "\n",
    "In order to build a representation for each paper, the following libraries will be used:\n",
    "\n",
    "- spaCy: https://spacy.io/\n",
    "- scispaCy: https://allenai.github.io/scispacy/\n",
    "\n",
    "The language model named`en_core_sci_sm` will be used, which has been trained with a corpus of biomedical text with a vocabulary of more than 100.000 words.\n",
    "In case of needing a model with a larger vocabulary, there are some others available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the biomedical language pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "nlp = en_core_sci_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a paper to showcase spacy's features\n",
    "sample_paper = list(pageranks.keys())[0]\n",
    "sample_text = \"\\n\".join([ paragraph[\"text\"] for paragraph in sample_paper._file_contents[\"body_text\"]])\n",
    "sample_text\n",
    "\n",
    "doc = nlp(sample_text, disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "doc[17].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document tokenized by the `spacy` pipeline is displayed.\n",
    "An interesting thing about using `spacy` with the pretrained language model is that it automatically computes document and token representations vectors.\n",
    "It's a pending task to find out which language model architecture it's used to compute those vectors.\n",
    "\n",
    "A relevant aspect that influences downstream tasks is the number of out-of-vocabulary (OOV) tokens.\n",
    "The following cell makes a quick inspection over a sample paper counting the number of OOV tokens.\n",
    "A continuación, se realizará una iteración sobre los tokens para detectarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov = 0\n",
    "for token in doc:\n",
    "    if token.is_oov and token.string != \"\\n\":\n",
    "        if token.string.endswith(\"virus\"):\n",
    "            print(token, \"not found\")\n",
    "        num_oov += 1\n",
    "    else:\n",
    "        if token.string.endswith(\"virus\"):\n",
    "            print(token, \"found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of out of vocabulary tokens: {num_oov} ({100 * num_oov / len(doc)}%).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that relevant tokens, such as *coronavirus* are included in the language model vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the mechanisms used to remove stopwords, punctuation, spaces, and extract the token's lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {token for token in doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_word_tokens = {token for token in doc if not (token.is_stop or token.is_punct or token.is_space)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(no_stop_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "The following cells will perform topic modelling experiments using the LDA technique.\n",
    "The `scikit-learn` implementation of this model will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's process all documents texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def process_papers_file_contents(papers):\n",
    "    texts = []\n",
    "    nlp = en_core_sci_sm.load()\n",
    "    for paper in progress_bar(papers):\n",
    "        text = \" \\n \".join([ paragraph[\"text\"] for paragraph in paper._file_contents[\"body_text\"]])\n",
    "        \"\"\"\n",
    "        NB.: for development speed purposes, the only document's attributes\n",
    "        considered for the topic modelling were the title and the abstract.\n",
    "        Should the text be included in other experiments, the following line\n",
    "        should be modified to include `{paper.text}`.\n",
    "        \"\"\"\n",
    "        texts.append(f\"{paper.title} \\n {paper.abstract}\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = process_papers_file_contents(list(pageranks.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peeking at the top 5 papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n=====\\n'.join(docs[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors storing the token occurrence count will be used as document representations.\n",
    "`tf-idf` vectors are purposefully not used because the document frequency normalization is already carried out by the LDA technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenizer(sentence):\n",
    "    tokens = []\n",
    "    for token in nlp(sentence, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
    "        # Se descartan números, stopwords, puntuación, espacio y tokens de largo 1\n",
    "        if not (token.like_num or token.is_stop or token.is_punct\n",
    "                or token.is_space or len(token) == 1):\n",
    "            tokens.append(token.lemma_)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=tokenizer, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = count_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse matrix is built rows one for each document, and columns: one for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, verbose=2, n_jobs=-1)\n",
    "lda = lda.fit(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of the following cells will display the most relevant tokens for each identified topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def topic_descriptors(topic_model, vectorizer, num_words):\n",
    "    res = {}\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(topic_model.components_):\n",
    "        res[topic_idx] =[feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = topic_descriptors(lda, count_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in descriptors:\n",
    "    print(f'Topic {topic_id}:', ', '.join(descriptors[topic_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset papers will be classified into the different previously modelled topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_classified = lda.transform(vectorized_docs)\n",
    "docs_classified[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the top-5 PageRank-sorted papers belonging to each topic are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_topics = docs_classified.argmax(1)\n",
    "topic_papers = defaultdict(list)\n",
    "all_papers = list(pageranks.keys())\n",
    "for idx, topic_id in enumerate(docs_topics):\n",
    "    topic_papers[topic_id].append(all_papers[idx])\n",
    "    \n",
    "for topic_id, papers in sorted(topic_papers.items(), key=lambda t: t[0]):\n",
    "    print(f\"Topic ID {topic_id}\")\n",
    "    sorted_papers = sorted(papers, reverse=True, key=lambda p: pageranks[p])\n",
    "    for paper in sorted_papers[:5]:\n",
    "        paper_as_markdown(paper)\n",
    "    #print(\"\\n\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
