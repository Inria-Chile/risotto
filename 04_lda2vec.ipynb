{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='left' style=\"width:29%;overflow:hidden;\">\n",
    "<a href='http://inria.fr'>\n",
    "<img src='https://github.com/lmarti/jupyter_custom/raw/master/imgs/inr_logo_rouge.png' alt='Inria logo' title='Inria logo'/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lda2vec\n",
    "\n",
    "lda2vec is an extension of word2vec and LDA that **jointly learns word, document and topic vectors**.\n",
    "\n",
    "lda2vec builds on top of the **skip-gram model of word2vec** to generate word vectors.\n",
    "\n",
    "With lda2vec, instead of using the word vector directly to **predict context words**, we leverage a **context vector** to make the predictions. This context vector is created as the sum of two other vectors: the **word vector** (generated by the skip-gram model) and the **document vector**.\n",
    "\n",
    "The **document vector is a weighted combination** of two components. A **document weight vector**, representing the weights of each topic in the document. And the **topic matrix**, representing each topic and its corresponding vector embedding.\n",
    "\n",
    "The power of lda2vec lies in the fact that simultaneously learns words embeddings, document representations and topic representations.\n",
    "\n",
    "![lda2vec](https://github.com/cemoody/lda2vec/raw/master/lda2vec_network_publish_text.gif)\n",
    "\n",
    "In this notebook, we'll attempt to train an lda2vec model on the CORD-19 dataset.\n",
    "First, we'll install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 19.0.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pyLDAvis in ./venv-risotto-notebooks/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: tensorflow in ./venv-risotto-notebooks/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: chainer in ./venv-risotto-notebooks/lib/python3.7/site-packages (7.4.0)\n",
      "Requirement already satisfied: keras in ./venv-risotto-notebooks/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: wheel>=0.23.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.9.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (1.18.4)\n",
      "Requirement already satisfied: scipy>=0.18.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (1.0.3)\n",
      "Requirement already satisfied: joblib>=0.8.4 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (0.14.1)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (2.11.2)\n",
      "Requirement already satisfied: numexpr in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: pytest in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (5.4.2)\n",
      "Requirement already satisfied: future in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: funcy in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (2.2.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (1.28.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions in ./venv-risotto-notebooks/lib/python3.7/site-packages (from chainer) (3.7.4.2)\n",
      "Requirement already satisfied: filelock in ./venv-risotto-notebooks/lib/python3.7/site-packages (from chainer) (3.0.12)\n",
      "Requirement already satisfied: setuptools in ./venv-risotto-notebooks/lib/python3.7/site-packages (from chainer) (40.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in ./venv-risotto-notebooks/lib/python3.7/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2020.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: wcwidth in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.9)\n",
      "Requirement already satisfied: packaging in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pytest->pyLDAvis) (20.3)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pytest->pyLDAvis) (8.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.6.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.14.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pyLDAvis) (3.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 19.0.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: lda2vec from git+https://github.com/cemoody/lda2vec.git@master#egg=lda2vec in ./venv-risotto-notebooks/lib/python3.7/site-packages (0.1)\n",
      "Requirement already satisfied: chainer>=1.5.1 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from lda2vec) (7.4.0)\n",
      "Requirement already satisfied: numpy>=1.10 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from lda2vec) (1.18.4)\n",
      "Requirement already satisfied: spacy>=0.9 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from lda2vec) (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.16.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from lda2vec) (1.4.1)\n",
      "Requirement already satisfied: sklearn in ./venv-risotto-notebooks/lib/python3.7/site-packages (from lda2vec) (0.0)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from chainer>=1.5.1->lda2vec) (3.11.3)\n",
      "Requirement already satisfied: six>=1.9.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from chainer>=1.5.1->lda2vec) (1.14.0)\n",
      "Requirement already satisfied: filelock in ./venv-risotto-notebooks/lib/python3.7/site-packages (from chainer>=1.5.1->lda2vec) (3.0.12)\n",
      "Requirement already satisfied: setuptools in ./venv-risotto-notebooks/lib/python3.7/site-packages (from chainer>=1.5.1->lda2vec) (40.8.0)\n",
      "Requirement already satisfied: typing-extensions in ./venv-risotto-notebooks/lib/python3.7/site-packages (from chainer>=1.5.1->lda2vec) (3.7.4.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (0.6.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (7.4.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from spacy>=0.9->lda2vec) (4.46.0)\n",
      "Requirement already satisfied: scikit-learn in ./venv-risotto-notebooks/lib/python3.7/site-packages (from sklearn->lda2vec) (0.22.2.post1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in ./venv-risotto-notebooks/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=0.9->lda2vec) (1.6.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=0.9->lda2vec) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=0.9->lda2vec) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=0.9->lda2vec) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=0.9->lda2vec) (2.9)\n",
      "Requirement already satisfied: joblib>=0.11 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from scikit-learn->sklearn->lda2vec) (0.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv-risotto-notebooks/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=0.9->lda2vec) (3.1.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "!pip install pyLDAvis tensorflow chainer keras\n",
    "!pip install --no-cache-dir git+https://github.com/cemoody/lda2vec.git@master#egg=lda2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we'll define the lda2vec model based on the examples in the library repository.\n",
    "A training function is also defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'move' from 'lda2vec.utils' (/home/rpalma/repos/risotto/risotto-notebooks/venv-risotto-notebooks/lib/python3.7/site-packages/lda2vec/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-25f250646720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlda2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlda2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlda2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedMixture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_top_words_per_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_coherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'move' from 'lda2vec.utils' (/home/rpalma/repos/risotto/risotto-notebooks/venv-risotto-notebooks/lib/python3.7/site-packages/lda2vec/utils.py)"
     ]
    }
   ],
   "source": [
    "# Ref.: https://github.com/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec_model.py\n",
    "# Ref.: https://github.com/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec_run.py\n",
    "\n",
    "# Author: Chris Moody <chrisemoody@gmail.com>\n",
    "# License: MIT\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "import shelve\n",
    "\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "import chainer.optimizers as O\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "from chainer import Chain\n",
    "import numpy as np\n",
    "\n",
    "from lda2vec import utils\n",
    "from lda2vec.utils import move\n",
    "from lda2vec import EmbedMixture, prepare_topics, print_top_words_per_topic, topic_coherence, dirichlet_likelihood\n",
    "\n",
    "\n",
    "class LDA2Vec(Chain):\n",
    "    def __init__(self, n_documents=100, n_document_topics=10,\n",
    "                 n_units=256, n_vocab=1000, dropout_ratio=0.5, train=True,\n",
    "                 counts=None, n_samples=15, word_dropout_ratio=0.0,\n",
    "                 power=0.75, temperature=1.0):\n",
    "        em = EmbedMixture(n_documents, n_document_topics, n_units,\n",
    "                          dropout_ratio=dropout_ratio, temperature=temperature)\n",
    "        kwargs = {}\n",
    "        kwargs['mixture'] = em\n",
    "        kwargs['sampler'] = L.NegativeSampling(n_units, counts, n_samples,\n",
    "                                               power=power)\n",
    "        super(LDA2Vec, self).__init__(**kwargs)\n",
    "        rand = np.random.random(self.sampler.W.data.shape)\n",
    "        self.sampler.W.data[:, :] = rand[:, :]\n",
    "        self.n_units = n_units\n",
    "        self.train = train\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.word_dropout_ratio = word_dropout_ratio\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def prior(self):\n",
    "        dl1 = dirichlet_likelihood(self.mixture.weights)\n",
    "        return dl1\n",
    "\n",
    "    def fit_partial(self, rdoc_ids, rword_indices, window=5,\n",
    "                    update_only_docs=False):\n",
    "        doc_ids, word_indices = move(self.xp, rdoc_ids, rword_indices)\n",
    "        pivot_idx = next(move(self.xp, rword_indices[window: -window]))\n",
    "        pivot = F.embed_id(pivot_idx, self.sampler.W)\n",
    "        if update_only_docs:\n",
    "            pivot.unchain_backward()\n",
    "        doc_at_pivot = rdoc_ids[window: -window]\n",
    "        doc = self.mixture(next(move(self.xp, doc_at_pivot)),\n",
    "                           update_only_docs=update_only_docs)\n",
    "        loss = 0.0\n",
    "        start, end = window, rword_indices.shape[0] - window\n",
    "        context = (F.dropout(doc, self.dropout_ratio) +\n",
    "                   F.dropout(pivot, self.dropout_ratio))\n",
    "        for frame in range(-window, window + 1):\n",
    "            # Skip predicting the current pivot\n",
    "            if frame == 0:\n",
    "                continue\n",
    "            # Predict word given context and pivot word\n",
    "            # The target starts before the pivot\n",
    "            targetidx = rword_indices[start + frame: end + frame]\n",
    "            doc_at_target = rdoc_ids[start + frame: end + frame]\n",
    "            doc_is_same = doc_at_target == doc_at_pivot\n",
    "            rand = np.random.uniform(0, 1, doc_is_same.shape[0])\n",
    "            mask = (rand > self.word_dropout_ratio).astype('bool')\n",
    "            weight = np.logical_and(doc_is_same, mask).astype('int32')\n",
    "            # If weight is 1.0 then targetidx\n",
    "            # If weight is 0.0 then -1\n",
    "            targetidx = targetidx * weight + -1 * (1 - weight)\n",
    "            target, = move(self.xp, targetidx)\n",
    "            loss = self.sampler(context, target)\n",
    "            loss.backward()\n",
    "            if update_only_docs:\n",
    "                # Wipe out any gradient accumulation on word vectors\n",
    "                self.sampler.W.grad *= 0.0\n",
    "        return loss.data\n",
    "    \n",
    "    \n",
    "def lda2vec_run():\n",
    "    gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "    cuda.get_device(gpu_id).use()\n",
    "    print(\"Using GPU \" + str(gpu_id))\n",
    "\n",
    "    data_dir = os.getenv('data_dir', '../data/')\n",
    "    fn_vocab = '{data_dir:s}/vocab.pkl'.format(data_dir=data_dir)\n",
    "    fn_corpus = '{data_dir:s}/corpus.pkl'.format(data_dir=data_dir)\n",
    "    fn_flatnd = '{data_dir:s}/flattened.npy'.format(data_dir=data_dir)\n",
    "    fn_docids = '{data_dir:s}/doc_ids.npy'.format(data_dir=data_dir)\n",
    "    fn_vectors = '{data_dir:s}/vectors.npy'.format(data_dir=data_dir)\n",
    "    vocab = pickle.load(open(fn_vocab, 'r'))\n",
    "    corpus = pickle.load(open(fn_corpus, 'r'))\n",
    "    flattened = np.load(fn_flatnd)\n",
    "    doc_ids = np.load(fn_docids)\n",
    "    vectors = np.load(fn_vectors)\n",
    "\n",
    "    # Model Parameters\n",
    "    # Number of documents\n",
    "    n_docs = doc_ids.max() + 1\n",
    "    # Number of unique words in the vocabulary\n",
    "    n_vocab = flattened.max() + 1\n",
    "    # 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "    clambda = 200.0\n",
    "    # Number of topics to fit\n",
    "    n_topics = int(os.getenv('n_topics', 20))\n",
    "    batchsize = 4096\n",
    "    # Power for neg sampling\n",
    "    power = float(os.getenv('power', 0.75))\n",
    "    # Intialize with pretrained word vectors\n",
    "    pretrained = bool(int(os.getenv('pretrained', True)))\n",
    "    # Sampling temperature\n",
    "    temperature = float(os.getenv('temperature', 1.0))\n",
    "    # Number of dimensions in a single word vector\n",
    "    n_units = int(os.getenv('n_units', 300))\n",
    "    # Get the string representation for every compact key\n",
    "    words = corpus.word_list(vocab)[:n_vocab]\n",
    "    # How many tokens are in each document\n",
    "    doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "    doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "    doc_lengths[doc_idx] = lengths\n",
    "    # Count all token frequencies\n",
    "    tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "    term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "    term_frequency[tok_idx] = freq\n",
    "\n",
    "    for key in sorted(locals().keys()):\n",
    "        val = locals()[key]\n",
    "        if len(str(val)) < 100 and '<' not in str(val):\n",
    "            print(key, val)\n",
    "\n",
    "    model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
    "                    n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                    n_samples=15, power=power, temperature=temperature)\n",
    "    if os.path.exists('lda2vec.hdf5'):\n",
    "        print(\"Reloading from saved\")\n",
    "        serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "    if pretrained:\n",
    "        model.sampler.W.data[:, :] = vectors[:n_vocab, :]\n",
    "    model.to_gpu()\n",
    "    optimizer = O.Adam()\n",
    "    optimizer.setup(model)\n",
    "    clip = chainer.optimizer.GradientClipping(5.0)\n",
    "    optimizer.add_hook(clip)\n",
    "\n",
    "    j = 0\n",
    "    epoch = 0\n",
    "    fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "    progress = shelve.open('progress.shelve')\n",
    "    for epoch in range(200):\n",
    "        data = prepare_topics(cuda.to_cpu(model.mixture.weights.W.data).copy(),\n",
    "                              cuda.to_cpu(model.mixture.factors.W.data).copy(),\n",
    "                              cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                              words)\n",
    "        top_words = print_top_words_per_topic(data)\n",
    "        if j % 100 == 0 and j > 100:\n",
    "            coherence = topic_coherence(top_words)\n",
    "            for j in range(n_topics):\n",
    "                print(j, coherence[(j, 'cv')])\n",
    "            kw = dict(top_words=top_words, coherence=coherence, epoch=epoch)\n",
    "            progress[str(epoch)] = pickle.dumps(kw)\n",
    "        data['doc_lengths'] = doc_lengths\n",
    "        data['term_frequency'] = term_frequency\n",
    "        np.savez('topics.pyldavis', **data)\n",
    "        for d, f in utils.chunks(batchsize, doc_ids, flattened):\n",
    "            t0 = time.time()\n",
    "            optimizer.zero_grads()\n",
    "            l = model.fit_partial(d.copy(), f.copy())\n",
    "            prior = model.prior()\n",
    "            loss = prior * fraction\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "            msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "                   \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "            prior.to_cpu()\n",
    "            loss.to_cpu()\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            rate = batchsize / dt\n",
    "            logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                        prior=float(prior.data), rate=rate)\n",
    "            print(msg.format(**logs))\n",
    "            j += 1\n",
    "        serializers.save_hdf5(\"lda2vec.hdf5\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dirichlet_likelihood'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a114f75a45a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlda2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/risotto/risotto-notebooks/venv-risotto-notebooks/lib/python3.7/site-packages/lda2vec/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdirichlet_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membed_mixture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dirichlet_likelihood'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lda2vec\n",
    "dir(lda2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above is caused because the library is written for Python 2.\n",
    "We've got to figure out some way to reuse it in a Python 3 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
