{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='left' style=\"width:29%;overflow:hidden;\">\n",
    "<a href='http://inria.fr'>\n",
    "<img src='https://github.com/lmarti/jupyter_custom/raw/master/imgs/inr_logo_rouge.png' alt='Inria logo' title='Inria logo'/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lda2vec\n",
    "\n",
    "lda2vec is an extension of word2vec and LDA that **jointly learns word, document and topic vectors**.\n",
    "\n",
    "lda2vec builds on top of the **skip-gram model of word2vec** to generate word vectors.\n",
    "\n",
    "With lda2vec, instead of using the word vector directly to **predict context words**, we leverage a **context vector** to make the predictions. This context vector is created as the sum of two other vectors: the **word vector** (generated by the skip-gram model) and the **document vector**.\n",
    "\n",
    "The **document vector is a weighted combination** of two components. A **document weight vector**, representing the weights of each topic in the document. And the **topic matrix**, representing each topic and its corresponding vector embedding.\n",
    "\n",
    "The power of lda2vec lies in the fact that simultaneously learns words embeddings, document representations and topic representations.\n",
    "\n",
    "![lda2vec](https://github.com/cemoody/lda2vec/raw/master/lda2vec_network_publish_text.gif)\n",
    "\n",
    "In this notebook, we'll attempt to train an lda2vec model on the CORD-19 dataset.\n",
    "First, we'll install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "!pip install pyLDAvis tensorflow chainer keras\n",
    "!pip install --no-cache-dir git+https://github.com/cemoody/lda2vec.git@master#egg=lda2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we'll define the lda2vec model based on the examples in the library repository.\n",
    "A training function is also defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref.: https://github.com/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec_model.py\n",
    "# Ref.: https://github.com/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec_run.py\n",
    "\n",
    "# Author: Chris Moody <chrisemoody@gmail.com>\n",
    "# License: MIT\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "import shelve\n",
    "\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "import chainer.optimizers as O\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "from chainer import Chain\n",
    "import numpy as np\n",
    "\n",
    "from lda2vec import utils\n",
    "from lda2vec.utils import move\n",
    "from lda2vec import EmbedMixture, prepare_topics, print_top_words_per_topic, topic_coherence, dirichlet_likelihood\n",
    "\n",
    "\n",
    "class LDA2Vec(Chain):\n",
    "    def __init__(self, n_documents=100, n_document_topics=10,\n",
    "                 n_units=256, n_vocab=1000, dropout_ratio=0.5, train=True,\n",
    "                 counts=None, n_samples=15, word_dropout_ratio=0.0,\n",
    "                 power=0.75, temperature=1.0):\n",
    "        em = EmbedMixture(n_documents, n_document_topics, n_units,\n",
    "                          dropout_ratio=dropout_ratio, temperature=temperature)\n",
    "        kwargs = {}\n",
    "        kwargs['mixture'] = em\n",
    "        kwargs['sampler'] = L.NegativeSampling(n_units, counts, n_samples,\n",
    "                                               power=power)\n",
    "        super(LDA2Vec, self).__init__(**kwargs)\n",
    "        rand = np.random.random(self.sampler.W.data.shape)\n",
    "        self.sampler.W.data[:, :] = rand[:, :]\n",
    "        self.n_units = n_units\n",
    "        self.train = train\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.word_dropout_ratio = word_dropout_ratio\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def prior(self):\n",
    "        dl1 = dirichlet_likelihood(self.mixture.weights)\n",
    "        return dl1\n",
    "\n",
    "    def fit_partial(self, rdoc_ids, rword_indices, window=5,\n",
    "                    update_only_docs=False):\n",
    "        doc_ids, word_indices = move(self.xp, rdoc_ids, rword_indices)\n",
    "        pivot_idx = next(move(self.xp, rword_indices[window: -window]))\n",
    "        pivot = F.embed_id(pivot_idx, self.sampler.W)\n",
    "        if update_only_docs:\n",
    "            pivot.unchain_backward()\n",
    "        doc_at_pivot = rdoc_ids[window: -window]\n",
    "        doc = self.mixture(next(move(self.xp, doc_at_pivot)),\n",
    "                           update_only_docs=update_only_docs)\n",
    "        loss = 0.0\n",
    "        start, end = window, rword_indices.shape[0] - window\n",
    "        context = (F.dropout(doc, self.dropout_ratio) +\n",
    "                   F.dropout(pivot, self.dropout_ratio))\n",
    "        for frame in range(-window, window + 1):\n",
    "            # Skip predicting the current pivot\n",
    "            if frame == 0:\n",
    "                continue\n",
    "            # Predict word given context and pivot word\n",
    "            # The target starts before the pivot\n",
    "            targetidx = rword_indices[start + frame: end + frame]\n",
    "            doc_at_target = rdoc_ids[start + frame: end + frame]\n",
    "            doc_is_same = doc_at_target == doc_at_pivot\n",
    "            rand = np.random.uniform(0, 1, doc_is_same.shape[0])\n",
    "            mask = (rand > self.word_dropout_ratio).astype('bool')\n",
    "            weight = np.logical_and(doc_is_same, mask).astype('int32')\n",
    "            # If weight is 1.0 then targetidx\n",
    "            # If weight is 0.0 then -1\n",
    "            targetidx = targetidx * weight + -1 * (1 - weight)\n",
    "            target, = move(self.xp, targetidx)\n",
    "            loss = self.sampler(context, target)\n",
    "            loss.backward()\n",
    "            if update_only_docs:\n",
    "                # Wipe out any gradient accumulation on word vectors\n",
    "                self.sampler.W.grad *= 0.0\n",
    "        return loss.data\n",
    "    \n",
    "    \n",
    "def lda2vec_run():\n",
    "    gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "    cuda.get_device(gpu_id).use()\n",
    "    print(\"Using GPU \" + str(gpu_id))\n",
    "\n",
    "    data_dir = os.getenv('data_dir', '../data/')\n",
    "    fn_vocab = '{data_dir:s}/vocab.pkl'.format(data_dir=data_dir)\n",
    "    fn_corpus = '{data_dir:s}/corpus.pkl'.format(data_dir=data_dir)\n",
    "    fn_flatnd = '{data_dir:s}/flattened.npy'.format(data_dir=data_dir)\n",
    "    fn_docids = '{data_dir:s}/doc_ids.npy'.format(data_dir=data_dir)\n",
    "    fn_vectors = '{data_dir:s}/vectors.npy'.format(data_dir=data_dir)\n",
    "    vocab = pickle.load(open(fn_vocab, 'r'))\n",
    "    corpus = pickle.load(open(fn_corpus, 'r'))\n",
    "    flattened = np.load(fn_flatnd)\n",
    "    doc_ids = np.load(fn_docids)\n",
    "    vectors = np.load(fn_vectors)\n",
    "\n",
    "    # Model Parameters\n",
    "    # Number of documents\n",
    "    n_docs = doc_ids.max() + 1\n",
    "    # Number of unique words in the vocabulary\n",
    "    n_vocab = flattened.max() + 1\n",
    "    # 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "    clambda = 200.0\n",
    "    # Number of topics to fit\n",
    "    n_topics = int(os.getenv('n_topics', 20))\n",
    "    batchsize = 4096\n",
    "    # Power for neg sampling\n",
    "    power = float(os.getenv('power', 0.75))\n",
    "    # Intialize with pretrained word vectors\n",
    "    pretrained = bool(int(os.getenv('pretrained', True)))\n",
    "    # Sampling temperature\n",
    "    temperature = float(os.getenv('temperature', 1.0))\n",
    "    # Number of dimensions in a single word vector\n",
    "    n_units = int(os.getenv('n_units', 300))\n",
    "    # Get the string representation for every compact key\n",
    "    words = corpus.word_list(vocab)[:n_vocab]\n",
    "    # How many tokens are in each document\n",
    "    doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "    doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "    doc_lengths[doc_idx] = lengths\n",
    "    # Count all token frequencies\n",
    "    tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "    term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "    term_frequency[tok_idx] = freq\n",
    "\n",
    "    for key in sorted(locals().keys()):\n",
    "        val = locals()[key]\n",
    "        if len(str(val)) < 100 and '<' not in str(val):\n",
    "            print(key, val)\n",
    "\n",
    "    model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
    "                    n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                    n_samples=15, power=power, temperature=temperature)\n",
    "    if os.path.exists('lda2vec.hdf5'):\n",
    "        print(\"Reloading from saved\")\n",
    "        serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "    if pretrained:\n",
    "        model.sampler.W.data[:, :] = vectors[:n_vocab, :]\n",
    "    model.to_gpu()\n",
    "    optimizer = O.Adam()\n",
    "    optimizer.setup(model)\n",
    "    clip = chainer.optimizer.GradientClipping(5.0)\n",
    "    optimizer.add_hook(clip)\n",
    "\n",
    "    j = 0\n",
    "    epoch = 0\n",
    "    fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "    progress = shelve.open('progress.shelve')\n",
    "    for epoch in range(200):\n",
    "        data = prepare_topics(cuda.to_cpu(model.mixture.weights.W.data).copy(),\n",
    "                              cuda.to_cpu(model.mixture.factors.W.data).copy(),\n",
    "                              cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                              words)\n",
    "        top_words = print_top_words_per_topic(data)\n",
    "        if j % 100 == 0 and j > 100:\n",
    "            coherence = topic_coherence(top_words)\n",
    "            for j in range(n_topics):\n",
    "                print(j, coherence[(j, 'cv')])\n",
    "            kw = dict(top_words=top_words, coherence=coherence, epoch=epoch)\n",
    "            progress[str(epoch)] = pickle.dumps(kw)\n",
    "        data['doc_lengths'] = doc_lengths\n",
    "        data['term_frequency'] = term_frequency\n",
    "        np.savez('topics.pyldavis', **data)\n",
    "        for d, f in utils.chunks(batchsize, doc_ids, flattened):\n",
    "            t0 = time.time()\n",
    "            optimizer.zero_grads()\n",
    "            l = model.fit_partial(d.copy(), f.copy())\n",
    "            prior = model.prior()\n",
    "            loss = prior * fraction\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "            msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "                   \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "            prior.to_cpu()\n",
    "            loss.to_cpu()\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            rate = batchsize / dt\n",
    "            logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                        prior=float(prior.data), rate=rate)\n",
    "            print(msg.format(**logs))\n",
    "            j += 1\n",
    "        serializers.save_hdf5(\"lda2vec.hdf5\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lda2vec\n",
    "dir(lda2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above is caused because the library is written for Python 2.\n",
    "We've got to figure out some way to reuse it in a Python 3 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
